{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eye_gaze.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "2jLn86rurBGf"
      },
      "cell_type": "markdown",
      "source": [
        "# Build Your Own MLP - Eye Gaze Estimation\n",
        "\n",
        "In the first part of the course you have implemented your own neural network using nothing but python, numpy and some brain power. In this additional part we will:\n",
        "\n",
        "1. Refactor and generalize the code\n",
        "2. Apply the learned concepts to the task of eye-gaze estimation.\n",
        "\n",
        "This notebook contains some gaps in the code that you are asked to fill in. We also release a solution file, but of course we encourage you to solve the tasks yourself before looking at the solutions.\n",
        "\n",
        "Eye-gaze information has many applications in Human-Computer Interaction such as improving user experience in everyday tasks, such as [reading](http://gbuscher.com/publications/BuscherBiedert10_readingRegions.pdf), or facilitate [gaze-based interaction](https://perceptual.mpi-inf.mpg.de/files/2014/07/majaranta14_apc.pdf). Eye-gaze also plays a crucial role in assisting users with motor-disabilities and can even be used to infer cognitive state such as cognitive load. Recently, deep-learning based gaze estimation was used for [unsupervised eye contact detection](https://perceptual.mpi-inf.mpg.de/files/2017/05/zhang17_uist.pdf) in everyday scenarios.\n",
        "\n",
        "The eye gaze data we'll be using is based on [UnityEyes](https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/), a synthetic eyes dataset. Note that this is a clean and simple dataset as far as eye gaze estimation goes. The real-world task (real images, uncontrolled environmental conditions) is much more challenging and comprehensively solving this is an active area of research and would go beyond the scope of this notebook."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "d4LMtG5Zn_-q",
        "outputId": "77e19806-15f4-4c86-b403-b6f6d5d246ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!if [ ! -f eye_data.h5 ]; then wget -nv https://github.com/jtj21/ComputationalInteraction18/blob/master/Otmar/data/eye_data.h5?raw=true -O eye_data.h5; fi\n",
        "  \n",
        "import h5py\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-02-22 16:32:47 URL:https://raw.githubusercontent.com/jtj21/ComputationalInteraction18/master/Otmar/data/eye_data.h5 [37401200/37401200] -> \"eye_data.h5\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jZvNG2wtrcNn"
      },
      "cell_type": "markdown",
      "source": [
        "With the necessary data and libraries imported, let's create split our dataset into training, validation and test sets."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2lnkOoEbV2_Q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load in our data\n",
        "with h5py.File('eye_data.h5', 'r') as h5f:\n",
        "\n",
        "  train_x = h5f['train/x_small'][:]\n",
        "  train_y = h5f['train/y'][:]\n",
        "\n",
        "  validation_x = h5f['validation/x_small'][:]\n",
        "  validation_y = h5f['validation/y'][:]\n",
        "\n",
        "  test_x = h5f['test/x_small'][:]\n",
        "  test_y = h5f['test/y'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FjQ76AJur1Dd"
      },
      "cell_type": "markdown",
      "source": [
        "# Task definition and metric\n",
        "\n",
        "We will apply the concepts we previously learned to build a simple neural network for directly regressing the gaze direction from single eye-images. \n",
        "For this purpose we will represent gaze direction by pitch and yaw in radians. \n",
        "\n",
        "Since pitch and yaw angles are difficult to interpret we will also define a more intuitive *angular* error metric based on [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity#Angular_distance_and_similarity). Angular distance in this case, is a single scalar value which would describe how many degrees the eyeball would need to turn to face match the estimated gaze direction.\n",
        "\n",
        "In addition we will define some helper functions to compute the metrics and a method to visualize our results."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BIqkl90lr43F",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def angular_error(X, y):\n",
        "    \"\"\"Calculate angular error (via cosine similarity).\"\"\"\n",
        "\n",
        "    def pitchyaw_to_vector(pitchyaws):\n",
        "        \"\"\"Convert given pitch and yaw angles to unit gaze vectors.\"\"\"\n",
        "        n = pitchyaws.shape[0]\n",
        "        sin = np.sin(pitchyaws)\n",
        "        cos = np.cos(pitchyaws)\n",
        "        out = np.empty((n, 3))\n",
        "        out[:, 0] = np.multiply(cos[:, 0], sin[:, 1])\n",
        "        out[:, 1] = sin[:, 0]\n",
        "        out[:, 2] = np.multiply(cos[:, 0], cos[:, 1])\n",
        "        return out\n",
        "\n",
        "    a = pitchyaw_to_vector(y) \n",
        "    b = pitchyaw_to_vector(X)\n",
        "\n",
        "    ab = np.sum(np.multiply(a, b), axis=1)\n",
        "    a_norm = np.linalg.norm(a, axis=1)\n",
        "    b_norm = np.linalg.norm(b, axis=1)\n",
        "\n",
        "    # Avoid zero-values (to avoid NaNs)\n",
        "    a_norm = np.clip(a_norm, a_min=1e-7, a_max=None)\n",
        "    b_norm = np.clip(b_norm, a_min=1e-7, a_max=None)\n",
        "\n",
        "    similarity = np.divide(ab, np.multiply(a_norm, b_norm))\n",
        "\n",
        "    return np.arccos(similarity) * (180.0 / np.pi)\n",
        "\n",
        "\n",
        "def predict_and_calculate_mean_error(nn, x, y):\n",
        "    \"\"\"Calculate mean error of neural network predictions on given data.\"\"\"\n",
        "    n, _, _ = x.shape\n",
        "    predictions = nn.predict(x.reshape(n, -1)).reshape(-1, 2)\n",
        "    labels = y.reshape(-1, 2)\n",
        "    errors = angular_error(predictions, labels)\n",
        "    return np.mean(errors)\n",
        "\n",
        "\n",
        "def predict_and_visualize(nn, x, y):\n",
        "    \"\"\"Visualize errors of neural network on given data.\"\"\"\n",
        "    %matplotlib inline\n",
        "    nr, nc = 1, 12\n",
        "    n = nr * nc\n",
        "    fig = plt.figure(figsize=(12, 2.))\n",
        "    predictions = nn.predict(x[:n, :].reshape(n, -1))\n",
        "    for i, (image, label, prediction) in enumerate(zip(x[:n], y[:n], predictions)):\n",
        "        plt.subplot(nr, nc, i + 1)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        error = angular_error(prediction.reshape(1, 2), label.reshape(1, 2))\n",
        "        plt.title('%.1f' % error, color='g' if error < 7.0 else 'r')\n",
        "        plt.gca().get_xaxis().set_visible(False)\n",
        "        plt.gca().get_yaxis().set_visible(False)\n",
        "    plt.tight_layout(pad=0.0)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "f-695jpZsRID"
      },
      "cell_type": "markdown",
      "source": [
        "# Network structure\n",
        "\n",
        "We can now begin defining our neural network, a simple multi-layer perceptron (MLP) with just a single hidden layer. This time we'll refactor the code a little to make it more configurable than in the first part of the tutorial.\n",
        "\n",
        "\n",
        "## Activation function\n",
        "First, we define a new activation function known as **Rectified Linear Unit** or **ReLU** and its derivative.\n",
        "\n",
        "\n",
        "> **_TASK 2A_**\n",
        "> Implement the ReLU function and its derivative below. Hint: Maybe [this](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) helps."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Og5HRBXUtFd1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ReLU(x):\n",
        "    \"\"\"Computes the Rectified Linear Unit function.\"\"\"\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "def ReLU_(x):\n",
        "    \"\"\"Computes the derivative of the ReLU function.\"\"\"\n",
        "    # TODO\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "h3yJoyHUtLIc"
      },
      "cell_type": "markdown",
      "source": [
        "The ReLU and its derivative look like this:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6kna6UyztPhE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.linspace(-2., 2., num=400)\n",
        "relu = ReLU(x)\n",
        "relu_prime = ReLU_(relu)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, relu, label=\"ReLU\")\n",
        "plt.plot(x, relu_prime, label=\"ReLU prime\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.xlim([-2, 2])\n",
        "plt.ylim([-1, 2])\n",
        "plt.legend(prop={'size' : 16})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PYZjgBzHuCG-"
      },
      "cell_type": "markdown",
      "source": [
        "Note how the ReLU activation function only passes through positive input values, rejecting all negative input values. This simple *non-linear* function can be stacked to create highly complex non-linear models. In addition, the very definite nature of its decision results in a so-called *sparsification* of a neural network, yielding quicker training times. That is, neurons are quickly selected to be \"killed off\", leaving just useful neurons active. See [this page](https://cs231n.github.io/neural-networks-1/) for a concise introduction to various activation functions and their pros and cons.\n",
        "\n",
        "We now define our error metric, the mean-squared error as the following:\n",
        "\n",
        "$$MSE\\left(y,\\hat{y}\\right) = \\frac{1}{n} \\sum_{i=0}^n \\left(y_i - \\hat{y}_i\\right)^2$$\n",
        "\n",
        "where there are $n$ output values in the ground-truth output $y$ and predicted output $\\hat y$. \n",
        "\n",
        "> **_TASK 2B_**\n",
        "> Implement the mean-squared error (MSE) metric below. Hint: You may consider using [np.square](https://docs.scipy.org/doc/numpy/reference/generated/numpy.square.html) and [np.mean](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cJmmeOCYvrXT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def MSE(Y, YH):\n",
        "    \"\"\"Compute elementwise mean square error between two matrices.\"\"\"\n",
        "    # TODO\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-i8OJzo3v29C"
      },
      "cell_type": "markdown",
      "source": [
        "We implement our neural network as a class, `NNRegressor`."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MRPRvDjnpnbl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NNRegressor:\n",
        "\n",
        "    def __init__(self, n_outputs, n_features, n_hidden_units=30,\n",
        "                 l2_reg=0.0, epochs=500, learning_rate=0.01,\n",
        "                 batch_size=10, random_seed=None):\n",
        "\n",
        "        if random_seed:\n",
        "            np.random.seed(random_seed)\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_features = n_features\n",
        "        self.n_hidden_units = n_hidden_units\n",
        "        self.w1, self.w2 = self._init_weights()\n",
        "        self.l2_reg = l2_reg\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Truncated normal for weights initialization\n",
        "        w1 = np.random.normal(0.0, 0.01, \n",
        "                              size=self.n_hidden_units * (self.n_features + 1))\n",
        "        w1 = np.clip(w1, -0.01, 0.01)\n",
        "        w1 = w1.reshape(self.n_hidden_units, self.n_features + 1)\n",
        "        w1[:, 0] = 1e-5  # Constant bias initialization\n",
        "        w2 = np.random.normal(0.0, 0.01, \n",
        "                              size=self.n_outputs * (self.n_hidden_units + 1))\n",
        "        w2 = np.clip(w2, -0.01, 0.01)\n",
        "        w2 = w2.reshape(self.n_outputs, self.n_hidden_units + 1)\n",
        "        w2[:, 0] = 1e-5  # Constant bias initialization\n",
        "        return w1, w2\n",
        "\n",
        "    def _add_bias_unit(self, X, how='column'):\n",
        "        if how == 'column':\n",
        "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
        "            X_new[:, 1:] = X\n",
        "        elif how == 'row':\n",
        "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
        "            X_new[1:, :] = X\n",
        "        return X_new\n",
        "\n",
        "    def _error(self, y, output):\n",
        "        return MSE(y, output)\n",
        "\n",
        "    def _backprop_step(self, X, y):\n",
        "        net_hidden, act_hidden, net_out = self._forward(X)\n",
        "        grad1, grad2 = self._backward(X, net_hidden, act_hidden, net_out, y)\n",
        "        \n",
        "        # regularize\n",
        "        grad1[:, 1:] += 2 * self.l2_reg * self.w1[:, 1:]\n",
        "        grad2[:, 1:] += 2 * self.l2_reg * self.w2[:, 1:]\n",
        "        \n",
        "        error = self._error(y, net_out)\n",
        "        return error, grad1, grad2\n",
        "\n",
        "    def predict(self, X):\n",
        "        net_hidden, act_hidden, net_out = self._forward(X)\n",
        "        return net_out\n",
        "\n",
        "    def fit(self, X_train, y_train, X_test, y_test):\n",
        "        training_errors = []\n",
        "        testing_errors = []\n",
        "\n",
        "        self.error_ = []\n",
        "        for i in range(self.epochs):\n",
        "            n_batches = int(X_train.shape[0] / self.batch_size)\n",
        "            X_mb = np.array_split(X_train, n_batches)\n",
        "            y_mb = np.array_split(y_train, n_batches)\n",
        "            \n",
        "            epoch_errors = []\n",
        "\n",
        "            for Xi, yi in zip(X_mb, y_mb):\n",
        "                batch_size = Xi.shape[0]\n",
        "                \n",
        "                # update weights\n",
        "                error, grad1, grad2 = self._backprop_step(Xi.reshape(batch_size, -1), yi)\n",
        "                epoch_errors.append(error)\n",
        "                self.w1 -= (self.learning_rate * grad1)\n",
        "                self.w2 -= (self.learning_rate * grad2)\n",
        "            mean_epoch_errors = np.mean(epoch_errors)\n",
        "            self.error_.append(mean_epoch_errors)\n",
        "\n",
        "            # Evaluate errors and visualize progress\n",
        "            if i % 5 == 0:\n",
        "                batch_train_error = predict_and_calculate_mean_error(self, Xi, yi)\n",
        "                training_errors.append([i + 1, batch_train_error])\n",
        "                if i % 10 == 0:\n",
        "                    mean_test_error = predict_and_calculate_mean_error(self, X_test, y_test)\n",
        "                    testing_errors.append([i + 1, mean_test_error])\n",
        "                    print('Epoch %d> mean test error: %f degrees' % (i + 1, mean_test_error))\n",
        "                    predict_and_visualize(self, X_test, y_test)\n",
        "\n",
        "        # Now plot a graph of error progression\n",
        "        training_errors = np.asarray(training_errors)\n",
        "        testing_errors = np.asarray(testing_errors)\n",
        "        plt.plot(training_errors[:, 0], training_errors[:, 1], 'g-*', label='train')\n",
        "        plt.plot(testing_errors[:, 0], testing_errors[:, 1], 'b-*', label='test')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Angular Gaze Error')\n",
        "\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xpVdCgWKtEPl"
      },
      "cell_type": "markdown",
      "source": [
        "The definition of this class is currently missing the `NNRegressor._forward` and `NNRegressor._backward` method definitions.\n",
        "\n",
        "Please fill this in below, being mindful of the dimensions involved. \n",
        "Note that biases can be pre-pended to the weights. Here is a pictorial representation of this idea:\n",
        "\n",
        "![NN with pre-pended bias node](https://i.imgur.com/u7RsiQR.png)\n",
        "\n",
        "> **_TASK 2C_** Implement `nn_forward_pass` which performs a forward pass through the neural network. Hint: You may need to use [np.ndarray.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.dot.html) for matrix multiplication and [np.ndarray.T](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.T.html) for matrix transpose. You will need to use the `NNRegressor._add_bias_unit` method to pad weight matrices with bias rows (or columns)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VQEeeNv5KAFJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nn_forward_pass(self, X):\n",
        "    \"\"\"Perform a forward pass of input data through this neural network.\n",
        "    \n",
        "    Note: The output of every step of this forward pass must be cached to be \n",
        "          used for the backward-pass. The backward-pass updates the neural \n",
        "          network weight and bias parameters.\n",
        "    \n",
        "    Params:\n",
        "        X: input data of shape (N x F)\n",
        "\n",
        "    Neural Network Weights:\n",
        "        self.w1: weight parameters of shape (H x F+1)\n",
        "        self.w2: weight parameters of shape (O x H+1)\n",
        "        \n",
        "    Legend:\n",
        "        N: Number of input data entries\n",
        "        F: Number of features\n",
        "        H: Number of neurons in hidden layer\n",
        "        O: Number of output values\n",
        "    \"\"\"\n",
        "    ## First step: ReLU(X * W1)\n",
        "    # Adjust input data to be of shape (N x F+1)\n",
        "    net_input_padded = None\n",
        "    \n",
        "    # Calculate hidden layer output of shape (N x H)\n",
        "    net_hidden = None\n",
        "    \n",
        "    # Calculate hidden layer activations of shape (N x H)\n",
        "    act_hidden = None\n",
        "    \n",
        "    ## Second step: X * W2\n",
        "    # Adjust activations to be of shape (N x H+1)\n",
        "    act_hidden_padded = None\n",
        "    \n",
        "    # Calculate neural network output of shape (N x O)\n",
        "    net_out = None\n",
        "    \n",
        "    return net_hidden, act_hidden, net_out\n",
        "\n",
        "NNRegressor._forward = nn_forward_pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5__uTmbnDIA2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now feed our inputs through the neural network to calculate the output.\n",
        "\n",
        "In order to be able to update our weight and bias matrices, we must now calculate gradients with respect to the output predictions (`net_out`) and ground-truth labels (`y`).\n",
        "\n",
        "> **_TASK 2D_**\n",
        "> Implement `nn_gradient_calculations` which calculates gradients for a backward-pass through the neural network, resulting in the updating of weights and biases. Hint: You may need to use the `ReLU_` function in addition to the functions you have used before. Otherwise you can find inspiration from the MLP in the XOR example."
      ]
    },
    {
      "metadata": {
        "id": "weeNbS6jDIA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nn_gradient_calculations(self, net_input, net_hidden, act_hidden, net_out, y):\n",
        "    \"\"\"Calculate gradients for a backward pass through this neural network.\n",
        "    \n",
        "    Params:\n",
        "        net_input: input data of shape (N x F)\n",
        "        net_hidden: output of hidden layer (N x H)\n",
        "        act_hidden: activations of hidden layer (N x H)\n",
        "        net_out: output of neural network (N x O)\n",
        "        y: ground-truth labels (N x O)\n",
        "\n",
        "    Neural Network Weights:\n",
        "        self.w1: weight parameters of shape (H x F+1)\n",
        "        self.w2: weight parameters of shape (O x H+1)\n",
        "        \n",
        "    Legend:\n",
        "        N: Number of input data entries\n",
        "        F: Number of features\n",
        "        H: Number of neurons in hidden layer\n",
        "        O: Number of output values\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate error residual (N x O)\n",
        "    de_do = 2*(net_out - y)\n",
        "    \n",
        "    #---#\n",
        "    \n",
        "    # Calculate derivative of output w.r.t w2 (N x H+1)\n",
        "    do_dw2 = None\n",
        "    \n",
        "    # Calculate gradient w.r.t self.w2 (O x H+1)\n",
        "    de_dw2 = de_do.T.dot(do_dw2)\n",
        "    \n",
        "    #---#\n",
        "    \n",
        "    # Calculate derivative of output w.r.t hidden layer activations (O x H+1)\n",
        "    # Remember: o = a * w2\n",
        "    do_da = None\n",
        "    \n",
        "    # Calculate derivative of hidden layer activations w.r.t hidden layer output (N x H+1)\n",
        "    # Remember: a = ReLU(h) but with bias-padding\n",
        "    da_dh = None\n",
        "    \n",
        "    # Calculate derivative of hidden layer output w.r.t w1 (N x F+1)\n",
        "    # Remember: h = x * w1\n",
        "    dh_dw1 = None\n",
        "    \n",
        "    # Calculate gradient w.r.t self.w1 (H x F+1)\n",
        "    de_dw1 = (de_do.dot(do_da) * da_dh).T.dot(dh_dw1)[1:, :]\n",
        "\n",
        "    return de_dw1, de_dw2\n",
        "\n",
        "NNRegressor._backward = nn_gradient_calculations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ga36qbB9KLIt"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and prediction\n",
        "\n",
        "Let's test if our implementation works.\n",
        "\n",
        "With the following code, you can create a neural network and train it using our eye-gaze data. While training, you will be able to see visually how well the optimization is proceeding via inspecting a select number of eye-images and their associated training error. After the training stops, you will see a graph summary of how your training and test errors progressed.\n",
        "\n",
        "Look out for the following phenomena:\n",
        "\n",
        "* **Overfitting**. When the network learns too specific patterns of the training data, it becomes less capable of generalizing its predictive capabilities to unseen test data. Or in other words, the training error reduces noticably without a noticable decrease in the test error.\n",
        "* **Underfitting**. When the network is learning so little that it performs as well as it does on seen data (training) and unseen data (test).\n",
        "\n",
        "In addition, pay attention to the speed of the training process. Which parameters should be tweaked to allow for quicker training without overfitting?\n",
        "\n",
        "> **_TASK 2E_** Try to beat your peers in the task of eye gaze estimation!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qrzyL_tjsGsx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A neural network should be trained until the training and test\n",
        "# errors plateau, that is, they do not improve any more.\n",
        "epochs = 201\n",
        "\n",
        "# Having more neurons in a network allows for more complex \n",
        "# mappings to be learned between input data and expected outputs.\n",
        "# However, defining the function to be too complex can lead to \n",
        "# overfitting, that is, any function can be learned to memorize\n",
        "# training data.\n",
        "n_hidden_units = 64\n",
        "\n",
        "# Lower batch sizes can cause noisy training error progression,\n",
        "# but sometimes lead to better generalization (less overfitting\n",
        "# to training data)\n",
        "batch_size = 16\n",
        "\n",
        "# A higher learning rate makes training faster, but can cause\n",
        "# overfitting\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Increase to reduce over-fitting effects\n",
        "l2_regularization_coefficient = 0.0001\n",
        "\n",
        "N_FEATURES = len(train_x[0, :].flatten())\n",
        "N_OUTPUTS = train_y.shape[1]\n",
        "\n",
        "nn = NNRegressor(n_outputs=N_OUTPUTS,\n",
        "                 n_features=N_FEATURES,\n",
        "                 n_hidden_units=n_hidden_units,\n",
        "                 l2_reg=l2_regularization_coefficient,\n",
        "                 epochs=epochs,\n",
        "                 learning_rate=learning_rate,\n",
        "                 batch_size=batch_size,\n",
        "                 random_seed=42)\n",
        "\n",
        "nn.fit(train_x, train_y, test_x, test_y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkLXefomDIA9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "What we are performing here is called *mini-batch gradient descent*. That is, we take a small sample of data points from our *training data*, and calculate our gradient update. Note that in the XOR exercise we performed *batch gradient descent* based on the full training dataset of 4 entries.\n",
        "\n",
        "What this means is that our method adds an additional hyperparameter to the training procedure in terms of how well or how quickly we can have our neural network learn. The hyperparameter is namely the *batch size*. If batch size is high, each gradient updates takes more computation but the gradient descent becomes more stable (and slow). If the batch size is too low, each gradient update may be so noisy that it becomes difficult to reach the optimum.\n",
        "\n",
        "When learning data-driven models, we often split our available data into training, validation and testing sub-sets. That is, we train our model on the training data, validate the performance of the model being trained occasionally, and finally produce a test error which characterizes the generalization capability of the model. Low batch sizes have shown to improve generalization, so finding a \"sweet spot\" in terms of batch size can be important in some cases.\n",
        "\n",
        "![](https://i.imgur.com/6spauos.png)\n",
        "\n",
        "A more core consideration for the training of neural networks is learning rate. The graphic below should characterize the three scenarios quite well. Note that this refers to the training loss progression, not the test loss progression.\n",
        "\n",
        "![](https://i.imgur.com/QaikS9E.png)\n",
        "\n",
        "Adjusting the learning rate based on training loss progression alone is insufficient as you can see in the following *over-fitting* phenomenon. That is, our model performs increasingly better on training data, but cannot transfer or *generalize* its knowledge to unseen validation or test data. When this occurs, one should (among other actions) consider lowering both learning rate and batch size.\n",
        "\n",
        "![](https://i.imgur.com/QtMc8us.png)"
      ]
    },
    {
      "metadata": {
        "id": "IV0j0WU5DIA9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}